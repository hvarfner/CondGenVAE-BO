{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, grad, lax, random\n",
    "from jax.experimental import optimizers\n",
    "from jax.experimental import stax\n",
    "from jax.experimental.stax import (Dense, FanOut, Relu, Softplus, Sigmoid, Softmax,\n",
    "                                   BatchNorm, Conv, ConvTranspose, Flatten, LogSoftmax)\n",
    "import numpy.random as npr\n",
    "import itertools\n",
    "\n",
    "LATENT_SIZE=2\n",
    "IMAGE_SHAPE=(28, 28)\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp\n",
    "from data import load_mnist\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "encoder_init, encode = stax.serial(\n",
    "    Conv(32, (3,3), (1,1), padding='SAME'), Relu,\n",
    "    Conv(64, (3,3), (2,2), padding='SAME'), Relu,\n",
    "    Conv(64, (3,3), (1,1), padding='SAME'), Relu,\n",
    "    Conv(64, (3,3), (1,1), padding='SAME'), Relu,\n",
    "    Flatten,\n",
    "    Dense(32), Relu,\n",
    "    FanOut(2),\n",
    "    stax.parallel(Dense(LATENT_SIZE), stax.serial(Dense(LATENT_SIZE), Softplus)),\n",
    ")\n",
    "\n",
    "decoder_init, decode = stax.serial(\n",
    "    Dense(int(np.prod(IMAGE_SHAPE) / 4 * 64)), Relu,\n",
    "    #Reshape((14 ,14, 64)),\n",
    "    ConvTranspose(32, (3,3), (2,2), 'SAME'), Relu,\n",
    "    Conv(1, (3,3), (1,1), padding='SAME'), Sigmoid,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hvarfner/anaconda3/envs/jaxnew/lib/python3.9/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
      "/home/hvarfner/anaconda3/envs/jaxnew/lib/python3.9/site-packages/torchvision/datasets/mnist.py:67: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "/home/hvarfner/anaconda3/envs/jaxnew/lib/python3.9/site-packages/torchvision/datasets/mnist.py:57: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a3630ba6b033>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtrain_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtest_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mnum_complete_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleftover\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdivmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mnum_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_complete_batches\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleftover\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "step_size = 0.001\n",
    "num_epochs = 100\n",
    "batch_size = 256\n",
    "nrow, ncol = 10, 10  # sampled image grid size\n",
    "test_rng = random.PRNGKey(1)  # fixed prng key for evaluation\n",
    "train_images = load_mnist(train=True)\n",
    "test_images = load_mnist(train=True)\n",
    "num_complete_batches, leftover = divmod(train_images.shape[0], batch_size)\n",
    "num_batches = num_complete_batches + bool(leftover)\n",
    "\n",
    "imfile = os.path.join(os.path.join(os.getcwd(), \"tmp/\"), \"mnist_vae_{:03d}.png\")\n",
    "encoder_init_rng, decoder_init_rng = random.split(random.PRNGKey(2))\n",
    "_, encoder_init_params = encoder_init(encoder_init_rng, (batch_size, 1,) + IMAGE_SHAPE)\n",
    "_, decoder_init_params = decoder_init(decoder_init_rng, (batch_size, LATENT_SIZE))\n",
    "decoder_init_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hvarfner/anaconda3/envs/jaxnew/lib/python3.9/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
      "/home/hvarfner/anaconda3/envs/jaxnew/lib/python3.9/site-packages/torchvision/datasets/mnist.py:67: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "/home/hvarfner/anaconda3/envs/jaxnew/lib/python3.9/site-packages/torchvision/datasets/mnist.py:57: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unimplemented: DNN library is not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9fb68aecc885>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mopt_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitercount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mepoch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jaxnew/lib/python3.9/site-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_execute_compiled\u001b[0;34m(compiled, avals, handlers, kept_var_idx, *args)\u001b[0m\n\u001b[1;32m    891\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m           if x is not token and i in kept_var_idx))\n\u001b[0;32m--> 893\u001b[0;31m   \u001b[0mout_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m   \u001b[0mcheck_special\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxla_call_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandlers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_partition_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unimplemented: DNN library is not found."
     ]
    }
   ],
   "source": [
    "step_size = 0.001\n",
    "num_epochs = 100\n",
    "batch_size = 256\n",
    "batch_size = 256\n",
    "INPUT_SHAPE = (28, 28)\n",
    "\n",
    "def Reshape(new_shape):\n",
    "    \"\"\"Layer construction function for flattening all but the leading dim.\"\"\"\n",
    "    def init_fun(rng, input_shape):\n",
    "        output_shape = (input_shape[0],) + new_shape\n",
    "        return output_shape, ()\n",
    "    def apply_fun(params, inputs, **kwargs):\n",
    "        output_shape = (inputs.shape[0],) + new_shape\n",
    "        return jnp.reshape(inputs, output_shape)\n",
    "    return init_fun, apply_fun\n",
    "\n",
    "network_init, predict = stax.serial(\n",
    "    Reshape((1, 28, 28)),\n",
    "    Conv(64, (3,3), (2,2), padding='SAME'), Relu,\n",
    "    Flatten,\n",
    "    Dense(256), Relu,\n",
    "    Dense(10), LogSoftmax)\n",
    "\n",
    "test_rng = random.PRNGKey(1)  # fixed prng key for evaluation\n",
    "nw_rng = random.PRNGKey(1)\n",
    "\n",
    "opt_init, opt_update, get_params = optimizers.momentum(step_size, mass=0.9)\n",
    "\n",
    "\n",
    "def loss(params, batch):\n",
    "    inputs, targets = batch\n",
    "    preds = predict(params, inputs)\n",
    "    return -jnp.mean(jnp.sum(preds * targets, axis=1))\n",
    "\n",
    "def accuracy(params, batch):\n",
    "    inputs, targets = batch\n",
    "    target_class = jnp.argmax(targets, axis=1)\n",
    "    predicted_class = jnp.argmax(predict(params, inputs), axis=1)\n",
    "    return jnp.mean(predicted_class == target_class)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    rng = random.PRNGKey(0)\n",
    "\n",
    "    step_size = 0.001\n",
    "    num_epochs = 10\n",
    "    batch_size = 32\n",
    "    momentum_mass = 0.9\n",
    "    \n",
    "    train_images, train_labels = load_mnist(train=True)\n",
    "    test_images, test_labels = load_mnist(train=False)\n",
    "    trainlabels_onehot = np.zeros((len(train_labels), 10))\n",
    "    testlabels_onehot = np.zeros((len(test_labels), 10))\n",
    "    trainlabels_onehot[np.arange(len(trainlabels_onehot)), train_labels] = 1\n",
    "    testlabels_onehot[np.arange(len(testlabels_onehot)), test_labels] = 1\n",
    "    train_labels = trainlabels_onehot\n",
    "    test_labels = testlabels_onehot\n",
    "    \n",
    "    num_train = train_images.shape[0]\n",
    "    num_complete_batches, leftover = divmod(num_train, batch_size)\n",
    "    num_batches = num_complete_batches + bool(leftover)\n",
    "    \n",
    "    def data_stream():\n",
    "        rng = npr.RandomState(0)\n",
    "        while True:\n",
    "            perm = rng.permutation(num_train)\n",
    "            for i in range(num_batches):\n",
    "                batch_idx = perm[i * batch_size:(i + 1) * batch_size:]\n",
    "                yield train_images[batch_idx], train_labels[batch_idx]\n",
    "    \n",
    "    batches = data_stream()\n",
    "    opt_init, opt_update, get_params = optimizers.momentum(step_size, mass=momentum_mass)\n",
    "    \n",
    "    @jit\n",
    "    def update(i, opt_state, batch):\n",
    "        params = get_params(opt_state)\n",
    "        return opt_update(i, grad(loss)(params, batch), opt_state)\n",
    "    \n",
    "    _, init_params = network_init(nw_rng, (batch_size, 28, 28))\n",
    "    opt_state = opt_init(init_params)\n",
    "    itercount = itertools.count()\n",
    "    \n",
    "    print('Starting training!')\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        for _ in range(num_batches):\n",
    "            opt_state = update(next(itercount), opt_state, next(batches))\n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        params = get_params(opt_state)\n",
    "        train_acc = accuracy(params, (train_images, train_labels))\n",
    "        test_acc = accuracy(params, (test_images, test_labels))\n",
    "        print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "        print(\"Training set accuracy {}\".format(train_acc))\n",
    "        print(\"Test set accuracy {}\".format(test_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_mnist(train=True)[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxnew",
   "language": "python",
   "name": "jaxnew"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
